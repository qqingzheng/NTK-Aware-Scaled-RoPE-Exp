{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7\n",
    "from transformers import LlamaModel, LlamaForCausalLM, LlamaTokenizer, GenerationConfig\n",
    "from transformers.models.llama.modeling_llama import LlamaRotaryEmbedding\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRotaryEmbeddingExt(torch.nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=16384, base=10000, alpha=8, device=None):\n",
    "        super().__init__()\n",
    "        alpha = alpha\n",
    "        base = base * alpha ** (dim / (dim-2))\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "        # Build here to make `torch.jit.trace` work.\n",
    "        self.max_seq_len_cached = max_position_embeddings\n",
    "        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        # This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self.max_seq_len_cached = seq_len\n",
    "            t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)\n",
    "            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "            # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n",
    "            self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n",
    "            self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n",
    "        return (\n",
    "            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
    "            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
    "        )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\"chinese-alpaca-plus-7b-merged\", device_map=\"auto\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"chinese-alpaca-plus-7b-merged\", use_fast=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inject into Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject(alpha=1):\n",
    "    for layer in model.base_model.layers:\n",
    "        origin = layer.self_attn.rotary_emb\n",
    "        head_dim = model.config.hidden_size // model.config.num_attention_heads\n",
    "        injector = LlamaRotaryEmbeddingExt(head_dim, alpha=alpha, device=origin.inv_freq.device)\n",
    "        layer.self_attn.rotary_emb = injector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from rouge_chinese import Rouge\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "def eval(output, repeat_times, generate_len=128):\n",
    "    prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n\\n{output*repeat_times}\\n\\n 请在提取上面文本的摘要\\n\\n### Response:\\n\\n\"\n",
    "    tokenized_sources = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    result = model.generate(**tokenized_sources, max_new_tokens=64, generation_config=GenerationConfig(output_scores=False))\n",
    "    output_result = tokenizer.decode(result[0]).split(\"### Response:\\n\\n\")[-1]\n",
    "    return rouge.get_scores(' '.join(jieba.cut(output)), ' '.join(jieba.cut(output_result)))[0]['rouge-l']['f'], tokenized_sources['input_ids'].size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "test_alpha_list = (1, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_content = \"主要依赖于相关司法解释文件的出台，从而呈现了紧急状态下的“应急释法刑事治理模式”。\"\n",
    "for alpha in test_alpha_list:\n",
    "    data[alpha] = []\n",
    "    print(f\"### {alpha}\")\n",
    "    for i in range(1, 1000, 5):\n",
    "        inject(alpha)\n",
    "        f1, token = eval(test_content, i)\n",
    "        print(f\"{token}: {f1}\")\n",
    "        data[alpha].append((token, f1))\n",
    "        if f1 == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(np.array(data[1])[:, 0], np.array(data[1])[:, 1])\n",
    "plt.plot(np.array(data[2])[:, 0], np.array(data[2])[:, 1])\n",
    "plt.plot(np.array(data[4])[:, 0], np.array(data[4])[:, 1])\n",
    "plt.xlabel(\"tokens\")\n",
    "plt.ylabel(\"rouge-f f1 score\")\n",
    "plt.legend([\"alpha=1\", \"alpha=2\", \"alpha=4\"])\n",
    "plt.savefig(\"result.png\", dpi=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatlaw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
